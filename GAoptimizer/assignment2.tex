%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{mathtools}
\usepackage{amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{hyperref}
\usepackage{url}
\usepackage{numberedblock}
\usepackage{graphicx}

\hypersetup {
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,          % color of internal links (change box color with linkbordercolor)
    urlcolor=blue           % color of external links
}

\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\allsectionsfont{\normalfont\scshape}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{0pt} % Customize the height of the header

\usepackage{titlesec}% http://ctan.org/pkg/titlesec
\titleformat{\section}%
  [hang]% <shape>
  {\normalfont\bfseries\Large}% <format>
  {}% <label>
  {0pt}% <sep>
  {}% <before code>
\renewcommand{\thesection}{}% Remove section references...
\renewcommand{\thesubsection}{\arabic{subsection}}%... from subsections

%\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{CSCI 4360/6360 Data Science II} \\
\textsc{Department of Computer Science} \\
\textsc{University of Georgia} \\ [15pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.3cm] % Thin top horizontal rule
\huge Assignment 2: Guest Lecturer Edition \\ % The assignment title
\horrule{2pt} \\[0.4cm] % Thick bottom horizontal rule
}

\author{Aditya Shinde} % Your name

%\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------


\section*{Questions}
\setcounter{subsection}{0}

\subsection{Linear Regression \textbf{[20pts]}}
For this question, I watched a couple of videos on youtube about maximum liklihood estimation and sampling data points from probability distributions. \\

\subsection*{1}
If the error terms $\epsilon_{i}$ are assumed to be i.i.d and sampled from a Gaussian distribution,

$$\epsilon \sim N(0,\sigma^{2})$$
$$\epsilon=y-w^{T}x$$
$$ y \sim N(w^{T}x,\sigma^{2})$$

Similarly, If\\

$$\epsilon \sim Lap(0,b)$$
$$\epsilon=y-w^{T}x$$
$$ y \sim Lap(w^{T}x,b)$$

$$P(Y|x_{i};w)=\textrm{argmax}\Pi_{i} Lap(w^{T}x,b)$$
$$P(Y|x_{i};w)=\textrm{argmax}\Pi_{i} \frac{1}{2b} e^{(\frac{-|y-w^{T}x|}{b})}$$
$$\log P(Y|x_{i};w)=\textrm{argmax}\sum -\log 2b+\left( \frac{-|y-w^{T}x|}{b}\right)$$

Now, $\log 2b$ is a constant, \\
So we can write $J_{Lap}(w)$ as,\\

$$J_{Lap}(w)=\textrm{argmin}\sum \left( |y-w^{T}x|\right)$$

\subsection*{2}

If the noise terms are distributed as Gaussians, the loss function is in the form of squared errors. This places a very high penalty on outliers and thus the model is highly reactive towards them. In case of a Laplacian distribution, the loss function is the mean absolute error. This is less reactive to outliers. 

\subsection{Regularization \textbf{[30pts]}}

\subsection*{1}

As $\lambda$ decreases, the cost function penalises exploding weights to a lesser extent.\\
As $\lambda \to 0$ the cost function $J_{R}(\beta) \to J(\beta)$

As $\lambda$ increases, the regularization term becomes more dominant. It starts heavily penalising all weights. 
As $\lambda \to \inf$, the cost function $J_{R}(\beta) \sim \lambda||\beta||^{2}$\\
The model will only fit the intercept. 

\subsection*{2}

$$\beta_{\textrm{MAP}} = \textrm{argmax}_{\beta} \prod_{i = 1}^n P(Y_i | X_i; \beta) P(\beta)$$

$$ \log \beta_{\textrm{MAP}}= \textrm{argmax}_{\beta} \sum_{i=1}^{n}\left( \log P(Y_i | X_i; \beta)+ \log P(\beta)\right)$$

$$ \log \beta_{\textrm{MAP}}= \textrm{argmax}_{\beta} \sum_{i=1}^{n}\left(-(Y-W^{T}X)^{2}+ (-\log \frac{\lambda}{I\sigma^{2}} - \frac{\beta^{2}\lambda}{I\sigma^{2}})\right)$$

Getting rid of the constants.

$$\textrm{argmax}_{\beta}\log \beta_{\textrm{MAP}}=\textrm{argmax}_{\beta} \sum_{i=1}^{n}-\left( (Y-W^{T}X)^{2}+\frac{\beta^{2}\lambda}{I\sigma^{2}}\right)$$

If we assume unit variance, $\sigma^{2}=1$ and $I$ is an identity matrix. And multiply and divide right hand side by -1 to change $\textrm{argmax}$ to $\textrm{argmin}$.

$$\textrm{argmax}_{\beta}\log \beta_{\textrm{MAP}}=\textrm{argmin}_{\beta} \sum_{i=1}^{n}\left( (Y-W^{T}X)^{2}+\lambda\beta^{2}\right)$$

\subsection*{3}

Now in this case $\beta$ is a random variable with variance $\frac{I\sigma^{2}}{\lambda}$. \\
If $\lambda \to 0$ in this case, that means $Var[\beta] \to \inf$, that is variance tends to infinity. So the samples will be far away. \\
Similarly, if $\lambda \to \inf$, $Var[\beta] \to 0$ which means $p(\beta)$ will be a single point at $0$ and we would essentially be fitting only the intercept.

\subsection{Evolutionary computation}
\subsection*{1}
Generational genetic algorithms use specific distint generations as different sets of parameters. By keeping distinct generations across epochs, we make sure that none of the individuals from the previous generation are in the current generation. This is specially helpful when the cost function is dynamic. It can also be helpful to find multiple solutions.

Steady state GAs on the other hand use a replacement mechanism. The newer individuals which are generated using genetic operators are swapped with the weaker individuals of the current generation.

One of the advantages of steady state GA's is that they require less resources in terms of memory since the population size is constant. Generational GA's on the other hand need twice as much memory. But steady state GA's usually take more time to converge.
\subsection*{2}
Pittsburgh and Michigan classifers both work on the same principles of evolutionary computation. The distinction is in the role of a single individual. In the Pittsburgh classifiers, each individual is a complete solution consisting of many rules. In case Michigan classifiers, an individual is just an element of a rule based system. Many such individuals need to be combined to make a complete solution.

One of the advantages that Michigan classifers have is diversity. Since the individuals are made of elementary rules, they can be used with many problems easily as compared to Pittsburgh. 

\end{document}